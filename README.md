# ğŸš€ Fine-Tuning LLM for Tamil Story Generation

This project demonstrates fine-tuning of transformer-based language models for creative Tamil story generation. It includes two parts:

* ğŸ“˜ `Fine_Tune_LLM_Part_1.ipynb` â€“ Basic fine-tuning using Hugging Face Transformers
* ğŸ§ª `Fine_Tune_LLM_Part_2.py` â€“ Advanced fine-tuning with LoRA (Low-Rank Adaptation) for efficient training.  
  ğŸ”¬ LoRA helps reduce training cost and memory usage while maintaining performance.  
  ğŸ“„ Read more: [LoRA Research Paper](https://arxiv.org/abs/2106.09685)


---
# ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ Fine_Tune_LLM_Part_1.ipynb   # Basic fine-tuning using HuggingFace Trainer
â”œâ”€â”€ Fine_Tune_LLM_Part_2.py      # Full pipeline with dataset prep, LoRA, and model saving
â””â”€â”€ README.md                    # You're here!
```


# ğŸ“Œ Part 1: Basic LLM Fine-Tuning

* ğŸ”§ Model: llama-2-7b
* ğŸ“¦ Library: transformers
* ğŸ“Š Dataset: Custom via `load_dataset("your_dataset")`

## âœ¨ Key Steps

* Load tokenizer and model
* Tokenize dataset
* Define training configuration
* Use Hugging Face Trainer API
* Save the fine-tuned model

# âš¡ Part 2: LoRA-based Efficient Fine-Tuning

* ğŸ¯ Goal: Fine-tune distilgpt2 on Tamil stories
* ğŸ§  Dataset: `tniranjan/aitamilnadu_tamil_stories_no_instruct`
* ğŸ’¡ Optimization: LoRA + AdamW + Trainer API
* ğŸ” Uses Hugging Face Hub login via `.env`

## ğŸ”§ Key Libraries Used

* `transformers`
* `peft` (Parameter-Efficient Fine-Tuning)
* `datasets`, `torch`, `pandas`
* `huggingface_hub`, `python-dotenv`

## ğŸ“˜ Highlights

* Preprocessing and tokenization
* LoRA configuration via `LoraConfig`
* Custom training loop using `Trainer`
* Saves model to Google Drive (Colab integration)
* Generates creative Tamil stories using prompts like **à®’à®°à¯ à®¨à®¾à®³à¯**

# ğŸ“Œ How to Run

â–¶ï¸ Open Notebook in Colab

# ğŸ§  Concepts Demonstrated

* Transfer Learning in NLP
* Fine-tuning Causal Language Models
* Tokenization and padding
* Training configuration with `TrainingArguments`
* Efficient training using LoRA (low-rank matrix adaptation)
* Model saving/loading and inference
* Tamil language generation with Transformers

# ğŸ“¦ Requirements

```bash
pip install datasets pandas torch transformers[torch] python-dotenv peft
```

# ğŸ“‚ Outputs

* ğŸ”¤ Fine-tuned model saved to local or Google Drive
* ğŸ“œ Text generations in Tamil using seed prompts

## ğŸ“š Learning Resources

If you're exploring LoRA (Low-Rank Adaptation) or PEFT (Parameter-Efficient Fine-Tuning), here are some high-quality resources to deepen your understanding:

- ğŸ”¬ [LoRA: Low-Rank Adaptation of Large Language Models (Paper)](https://arxiv.org/abs/2106.09685)
- ğŸ¤— [Hugging Face PEFT Documentation](https://huggingface.co/docs/peft/index)
- ğŸ’¡ [Fine-Tuning vs. Parameter-Efficient Fine-Tuning â€“ Sebastian Raschka](https://sebastianraschka.com/blog/2023/peft.html)

These links complement the methods used in `Fine_Tune_LLM_Part_2.py`.


# ğŸ™Œ Acknowledgements

This repo demonstrates a practical application of Hugging Face Transformers and LoRA. Special thanks to open datasets and pretrained models shared on ğŸ¤— Hugging Face Hub.

# ğŸ‘¨â€ğŸ’» Author

**Lingesh S**
Interested in LLMs, RAG pipelines, and practical NLP

[GitHub](https://github.com/Lingesh-S) â€¢ [LinkedIn](https://linkedin.com/in/lingesh-s29)
